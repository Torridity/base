Architecture Overview
---------------------
In the following chapters basic components and concepts of the architecture of KIT Data Manager are described briefly. Basically, the presented components are responsible for 
abstracting from underlaying infrastructure and for providing a common service layer for integrating community-specific respository solutions.

[[FigureArchitecture]]
.Basic architecture of KIT Data Manager.
image::Architecture.png[Modules]

The <<FigureArchitecture,figure above>> shows a quite general view on the differnt layers and services provided by KIT Data Manager. The following table gives a short impression 
about the resposibilities of each service and in which chapter(s) the services are explained in detail.

[cols="m,n,o", options="header"]
|============================================================================================================================
|Service|Covered in chapter...|Short Description
|Data Sharing|<<ChapterAuthorization,Authorization>>|Authorizing access to data and functionality based on users, groups and roles.
|Metadata Management|<<ChapterMetadataManagement,Metadata Management>>|Managing and accessing different kinds of metadata.
|Search|<<ChapterMetadataManagement,Metadata Management>>,<<ChapterDataOrganization,Data Organization>>|Search for metadata.
|Staging|<<ChapterStaging,Staging>>, <<ChapterDataOrganization,Data Organization>>|Accessing and managing (file-based) data.
|Data Processing|Not available, yet.|Processing of data stored in a KIT Data Manager repository.
|Lifecycle Management|Not available, yet.|Services for managing a data lifecycle.
|Policy Enforcement|Not available, yet.|Services for enforcing policies on data.
|============================================================================================================================

Before describing each single component let's take a short look at the main idea behind KIT Data Manager: to provide
a generic architecture for building up repository systems for arbitrary scientific communities. To be able to cope with the tasks of a repository system (e.g. content preservation,
bit preservation, curation...) there is the need to think in structured, `Digital Object`-based rather than in unstructured, File-based dimensions. Of course, in many cases there 
are files managed by KIT Data Manager on the lowest layer (see <<ChapterDataOrganization,Data Organization>>) but they are only one possible representation of the content of a Digital Object. 
The Digital Object and everything related to it is described by a huge amount of metadata in order to enable software systems to interpret Digital Objects, their content and their 
provenance. However, while reading the following chapters bear always in mind that everything in KIT Data Manager is related to structured data entities called
Digital Objects consisting of metadata and data.

[[ChapterAuthorization]]
Authorization
~~~~~~~~~~~~~

One of the core components which concerns almost every part of KIT Data Manager is the Authorization. It is based on users, groups and an effective role and determines if the 
access to a secured resource (e.g. Digital Objects) or operation (e.g. adding users) is granted or not. Authorization decisions in KIT Data Manager are always based on a 
specific context the user is currently working in. This context consists of:

UserId::
   An internal user identifier. This identifier is unique for each user and is assigned during registration, e.g. via a Community-specific Web frontend. 
   For real world applications, the user identifier should be obtained from another, central source like an LDAP database in order 
   to provide a common source of authentification for data and metadata access.
GroupId::
   An internal group id. This idenfier is unique for each user group of a KIT Data Manager instance. Available groups can be managed by higher level services. 
   By default, there is only one group with the id `USERS` which contains all registered users.

The final part needed for authorization decisions is the `Role`. A role forms, together with UserId and GroupId, the `AuthorizationContext` that is used to access 
resources or functionalities. Currently, there are the following roles available:

[cols="m,n", options="header"]
|============================================================================================================================
|Role|Description
|NO_ACCESS|An AuthorizationContext with this role has no access to any operation or resource.
|MEMBERSHIP_REQUESTED|This is an intermediate role used in special cases. 
 Users with this role have requested the membership for e.g. a group but are not activated, yet.
|GUEST|Using an AuthorizationContext with the role `GUEST` grants read access to public accessible resources. 
 Modifications of resources are not allowed.
|MEMBER|An AuthorizationContext with this role can be used to read and modify resources. This role should be used in most cases.
|MANAGER|An AuthorizationContext with the role `MANAGER` can be used for operations that require special access to resources and 
functionalities on a group level, e.g. assign new memberships to a group or deleting resources owned by a specific group.
|ADMINISTRATOR|The role `ADMINISTRATOR` is used for operations that require special access on a global level. Typically, the 
administrator role should be used sparingly and only for a small amount of administrator users.
|============================================================================================================================

Out of these roles, each user has a maximum role `MAX_ROLE` which is defined globally and cannot be bypassed.

[NOTE]
The MAX_ROLE defines the highest role a user may possess. This role might be further restricted on group- or resource-level, so that a maximum role of MANAGER may result in a 
group role MEMBER. On the other hand it is not possible to gain a higher role than MAX_ROLE, which means, if the maximum role of a user is set to NO_ACCESS, the user is not 
allowed to do anything and won't be able to gain more permissions on group- or resource-level. 

To determine the actual role within a specific context, a user has a group-specific role and the MAX_ROLE. The effective role is the group role unless MAX_ROLE is smaller. 
In this case MAX_ROLE is the effective role.

Additionally, there can be resource-specific roles issued separately. By default, each resource with restricted access can be accessed by all members of the creator's group 
with the role MANAGER (read and modify). For sharing purposes it is also possible to issue additional permissions for another groups (called `ResourceReferences`) 
or to single users (called `Grants`). This allows a user who is no member of the creator's group to access a single resource with a specific role.

Apart from access to resources also access to operations can be restricted. For basic services and components of KIT Data Manager the restriction for operations is 
roughly oriented on the role definition presented above. This means, that the role GUEST is needed to perform read operations, MEMBER is needed for basic write operations
and the MANAGER role entitles the user to remove data (to a very limited extent).

[[ChapterMetadataManagement]]
Metadata Management 
~~~~~~~~~~~~~~~~~~~
The core of the metadata management of KIT Data Manager is the metadata model, which is presented in the <<MetadataModel,figure below>>. 

[[MetadataModel]]
.Metadata model of KIT Data Manager. An important point in this model is taken by the Digital Object which is, as part of the Base Metadata, providing the Digital Object ID (OID). This OID is used to refer to the according Digital Object by other linked entities, e.g. Staging or Data Organization Metadata.
image::MetaData.png[Metadata model]

The metadata model constists of three parts which can be described as follows:

Administrative Metadata::
   This category contains metadata elements that are mostly used internally by KIT Data Manager and its services. These elements have a fixed schema and are typically stored 
   in a relational database. One important part is the Base Metadata defining a guaranteed core set of metadata elements that are expected to be available for each and every 
   Digital Object managed by KIT Data Manager. Parts of the Base Metadata are adopted from the link:http://epubs.stfc.ac.uk/bitstream/485/csmdm.version-2.pdf[Core Scientific 
   Metadata Model (CSMD) 2.0], other parts of this model were skipped to reduce complexity and to allow more flexibility. An overview of all Base Metadata entities and how they 
   relate to each other is depicted in figure <<BaseMetadata,BaseMetadata>>. One can see the main entities 'Study', 'Investigation' and 'Digital Object' which also contains 
   the (Digital) Object Identifier also referenced as OID in this document. The OID identifies each Digital Object and can be used to link additional metadata entities to a 
   Digital Object, e.g. Staging Metadata or Sharing Metadata as shown in figure <<MetadataModel,MetadataModel>>. All administrative metadata elements may or may not be exposed 
   to the user by according service interfaces.
Data Organization Metadata::
   The Data Organization Metadata contains information on how data managed by KIT Data Manager is organized/structured and where it is located. Currently, only file-based 
   Data Organization is supported and all Data Organization information is stored in relational databases. For more information please refer to chapter 
   <<ChapterDataOrganization,Data Organization>>.
Content Metadata::
   The third part of the metadata model is the content metadata. Content metadata covers for example community-specific metadata providing detailed information about the 
   content of a Digital Object. For the sake of flexibility all content metadata related implementations are outsourced into a separate module called Enhanced Metadata Module 
   (see <<ChapterEnhancedMetadataArchitecture,Extension: Enhanced Metadata Module>>) that can be installed optionally to the basic KIT Data Manager distribution.
  
[[BaseMetadata]]
.The most important Base Metadata entities and their relations. The core entities, namely Study, Investigation and Digital Object, are highlighted. Furthermore, some of the relationships are simplified for reasons of clarity.
image::BaseMetadata.png[BaseMetadata, width="700px", align="center"]

Administrative and Data Organization Metadata of KIT Data Manager are stored in relational databases using well defined schemas and can be accessed using Java or REST service APIs.
For content metadata there is no fixed API or schema available as content metadata strongly differs depending on the community and supported use cases. Rather there are basic workflows
available that can be implemented in order to extract, register and retrieve content metadata. How access to this metadata is realized depending on the system(s) in which the metadata
are registered. 

[[ChapterStaging]]
Staging
~~~~~~~
The term `Staging` basically stands for the process of transferring data in or out KIT Data Manager, either to access data manually or automatically for computing purposes.
As KIT Data Manager aims to be able to build up repository systems for large-scale scientific experiment data, the data transfer needs a special focus. 
In contrast to traditional repository systems KIT Data Manager provides reasonable throughput in order to be able to cope with the huge amounts of data 
delivered by scientists. In addition, scientific data can be very diverse, e.g. hundred of thousands of small files vs. a handful of files in the order of terabyte.
Therefore, the process of Staging in case of an ingest (for downloads this process is carried out the other way around) is divided into two parts: 

Caching::
   Caching is the plain data transfer to a temporary storage system which is accessible in an optimal way depending on the requirement, e.g. throughput, security or 
   geographical location. To achieve best results transfers to the cache are carried out using native clients or custom, optimized transfer tools. The location where the 
   cached data can be stored is provided and set up by KIT Data Manager using pre-configured `StagingAccessPoints`. A `StagingAccessPoint` defines the protocol as well as 
   the local and remote base path/URL accessible by the repository system and the user. Details about StagingAccessPoints can be found in the `Programming KIT Data Manager` 
   or `Administration UI` chapters.
Archiving::
   During archiving the data from the cache is validated as far as possible, metadata might be extracted and transfer post-processing may take place. Afterwards, the data
   is copied from the cache to a managed storage area where the repository system is taking care of the data. As soon as the data is in the managed storage, it can be expected 
   to be safe. Local copies and cached data can be removed and repository workflows start taking care of the data.

[NOTE]
Authentification and authorization for data transfers to and from the cache is not covered by KIT Data Manager. This offers a huge level of flexibility and allows to customzie 
the data transfer to possible needs. However, it is still possible to use the same user database that is used to obtain KIT Data Manager UserIDs, e.g. an LDAP server. The only thing that
has to be ensured is that for data ingests the written data has to be accessible by KIT Data Manager and for downloads the data, written by KIT Data Manager, must be readable by the user 
who wants to access the data. Typically, this can be achieved by running KIT Data Manager as a privileged user or by handling access permission on a group level. 

As mentioned before, the transfer into the archive storage is much more than a simple copy operation. The process of initially providing data associated with a 
Digital Object is called `Ingest`. During Ingest, different steps like metadata extraction, checksumming or even processing steps might be performed. For download operations 
requested data is copied first from the archive to the cache and can then be downloaded by the user using an according StagingAccessPoint. Furthermore, this workflow can be also used to copy the data to an external 
location, e.g. for data processing. In order to be able to provide supplementary data (e.g. files containing metadata), a special folder structure was defined for each staging location:

.Staging Tree Structure
-----------------------------------------------
31/12/69  23:59         <DIR>    data       <1>
31/12/69  23:59         <DIR>    generated  <2>
31/12/69  23:59         <DIR>    settings   <3>
-----------------------------------------------
<1> Contains the user data, e.g. uploaded files or files for download.
<2> Contains generated data, e.g. extracted metadata or processing results
<3> Contains KIT Data Manager-related settings, e.g. credentials for third-party transfers.

<<Staging,The graphic>> below shows the general Staging workflow. At the beginning, the `Staging Service` is accessed using a `Transfer Service Interface`, which might be 
a commandline client, a Web portal or something comparable. In case of downloading data the Staging operation is scheduled and will data be made available asynchronously 
by the Staging Service as this preparation may take a while (e.g. when restoring data from tape). In case of an ingest the cache is prepared immediately. 
As soon as the preparation of the data transfer operation is finished, the data is accessible from the cache by a data consumer or can be transferred to the cache by a data producer. 
Both, cache and archive storage must be accessible by KIT Data Manager, at least one of them must be accessible (also) in a POSIX-like way. 

[[Staging]]
.Staging workflow for ingest operations with KIT Data Manager. After selecting the data (1) a Digital Object is registered (2) and a new ingest is scheduled. As soon as the transfer is prepared, the data can be transfered (3). Finally, the ingest is marked for finalization (4). During finalization the cached data is copied to the archive (5), the Data Organization is obtained and content metadata might be extracted automatically. Finally, extracted content metadata is made accessible e.g. by a search index (6).
image::Staging.png[Staging, width="600px", align="center"]

[[ChapterDataOrganization]]
Data Organization
~~~~~~~~~~~~~~~~~
The Data Organization is closely coupled with the Staging and holds information on how the data belonging to a Digital Object is organized and where it is located. 
In the most simple case, after ingesting a file tree the Data Organization reflects exactly the ingested tree including CollectionNodes representing folders and FileNodes representing the files.
This allows to restore the file tree in case of a download in the representation the user expects. In addition, there might be attributes linked to single Data Organization items, 
e.g. size or mime type of a tree node. In more sophisticated scenarios the Data Organization might be customized according to user needs. 
These customizations are called `Views`. Views can be useful, e.g. to group all files with the same type belonging to one Digital Object or to transform a Digital Object's data into another 
format but keep the data organization linked to the particular Digital Object.

[[DataOrganization]]
.The graphic shows exemplarily different views for the Data Organization of a Digital Object. On the left hand side the default view with all contained files is shown. The second view contains only the data files, the third view contains a compressed version of the default view. Finally, the view on the right hand side contains generated files with a preview of the images contained in the default view that can be mapped by their names to each other.
image::DataOrganization.png[DataOrganization, width="600px", align="center"]

For obtaining data represented in the Data Organization typically the Staging Service is used for transferring data to a location accessible using one of the configured StagingAccessPoints. 
Another alternative that can be used for smaller downloads and for synchronous access without including the asynchronous staging process the Data Organization REST service offers the direct
access to the content of single FileNodes and to CollectionsNodes by downloading all sub-nodes as a Zip archive. Please refer to the REST documentation for more details.

[[ChapterDataWorkflow]]
Data Workflows
~~~~~~~~~~~~~~

A very special feature that distinguishes KIT Data Manager from other research data repository systems is the ability to trigger data processing workflows. This allows to execute data workflows seamlessly integrated
into repository system workflows. The repository system takes care of transferring the data to the processing environment, monitoring the execution and ingesting the results back to the repository system
and link them to the input object(s). Furthermore, single processing tasks can be chained to construct complex data processing workflows. The Data Workflow module is based on three major entities:

Execution Environment::
   The Execution Environment is the physical environment where a single data workflow task is executed. It might be the local machine or a compute cluster. The access to an execution environment is 
   implemented using an appropriate `ExecutionEnvironmentHandler` taking care of the preparation of the application and the input data, the execution of the application and its monitoring 
   as well as the ingest of the results, the creation of links to the input Digital Objects and the cleanup. However, this process can be abstracted quite well so that different handler implementations
   only have to take care about the actual execution/submission of the application and the monitoring of its status. The data transfer can be fully covered in a generic way using the Staging Service of 
   KIT Data Manager by assigning a StagingAccessPoint to each execution environment.
Data Workflow Task Configuration:: 
   A Data Workflow Task Configuration describes a single task that can be executed alone or after a predecessor task to chain multiple tasks. An actual instance of a Data Workflow Task Configuration is
   just called Data Workflow task. A task configuration consists of basic metadata, e.g. name, version, description and keywords, a package URL, which is pointing to a ZIP file containing everything 
   needed to execute the task's application, e.g. libraries, executables and a wrapper script named 'run.sh', and fixed application arguments. For all tasks registered in a KIT Data Manager instance 
   the combination of name, version, application package URL and application arguments is unique. If any of these fields changes, a new task version or an entirely new task must be created. 
   This is mandatory in order to be able to collect reliable provenance information later on. As all tasks are executed by an according ExecutionEnvironmentHandler fully automatically, the successful 
   execution of the task in the targeted environment should be tested in beforehand before registering the task in the repository system. This excludes principle errors, e.g. missing dependencies.
   Due to this required effort (packaging and testing the application) Data Workflow Tasks are mainly interesting for tasks executed many times.
Environment Property::
   Finally, there are Environment Properties allowing to describe capabilities of an execution environment, e.g. the platform or existing libraries, and the requirements of a Data Workflow Task. 
   In both cases, environment properties can be chosen from a common pool of properties and before registering a task execution in an execution environment capabilities and requirements are 
   matched to determine whether an execution environment can principally handle a task. However, there is currently no mechanism to ensure that an execution environment is really
   providing a specific environment property, e.g. the defined platform or a software package.

After describing the major entities the question is, how they work together. The first point here is the actual application that will be executed as a Data Workflow Task. Such application must be packed 
in a Zip archive with the following structure:

.Application Package Structure 
-----------------------------------------------
31/12/69  23:59         <DIR>    app        <1>
31/12/69  23:59         123      run.sh     <2>
-----------------------------------------------
<1> Contains the user application, e.g. libraries, binaries and static configuration files.
<2> The execution wrapper script for setting up and calling the user application. 

The actual user application may or may not be located in a directory called `app` but it is recommended to achieve a clean cut between data workflow and user application. The execution wrapper consistes of 
two parts: a common part that is recommended for all wrapper scripts and a specific part for setting up and calling the actual user application. The base script looks as follows:

[source,sh]
--------------------------------------
#!/bin/sh

#Variable definition for accessing/storing data. The placeholder variables, e.g. ${data.input.dir}, are replaced by the workflow service using the 
#appropriate values for the according task execution and execution environment. 
export INPUT_DIR=${data.input.dir}
export OUTPUT_DIR=${data.output.dir}
export WORKING_DIR=${working.dir}
export TEMP_DIR=${temp.dir}

#Place environment checks here, if necessary, to allow debugging. However, if something is missing at this point, the process will fail 
#either way.

#Now, the execution of the user application starts. The variables above should be provided to the process in a proper way depending on 
#the kind of the process, e.g. for Java processes via -DINPUT_DIR=$INPUT_DIR 

#At this point, the user application is executed, e.g. via './app/MainBinary $@'. The argument $@ is recommended to forward all fixed command line arguments 
#provided by the task configuration and the dynamic arguments optionally provided for each specific task instance. Each user application should return
#a proper exit code (0 for success, any other value for error).

#Obtain the exit code of the process, print out a logging message for debugging, and exit the wrapper script using the exit code of the internal
#process to allow a proper handling by the workflow service. If the exit code is 0 the data ingest of all data stored in $OUTPUT_DIR is triggered.
#Otherwise, the task will remain in an error state and needs user interaction.
EXIT=$?
echo "Execution finished."
exit $EXIT
--------------------------------------
 
It is recommended to use this wrapper script as it allows to obtain the proper directories from the repository and to provide an exit code for proper callback. 
The sample script above shows one feature allowing the repository to provide information for the user application: the variable substitution. All variables are 
pointing to an absolute path within the execution environment. Available variables are:

[cols="m,n", options="header"]
|============================================================================================================================
|Variable|Content
|${data.input.dir}|The directory containing all staged input data. If multiple digital objects are input for one task, the data of all object will be located in this directory.
|${data.output.dir}|The directory that can be used to store output data. All data located in this directory will be ingested as a new digital object as soon as the processing has succeeded. 
|${working.dir}|The working directory where the application archive is extracted to. Furthermore, the execution settings, which can be provided for each task execution, 
are stored in a file `dataworkflow.properties`, which is also located in the working directory.
|${temp.dir}|A temporary directory where the user application can store intermediate data. The content of this directory will be removed during task cleanup.
|============================================================================================================================

By default, the Data Workflow Service replaces these variables only in the file `run.sh`. In order to enforce variable substitution in custom files, e.g. settings or other application-specific files,
an empty file named `dataworkflow_substitution` has to be placed in each directory where substitution should take place. Variables substitution will then be applied to all files in this 
directory with a size smaller than 10 MB.

The actual execution of a workflow task is covered by an associated execution environment handler. This handler executes each task in multiple phases which are the following: 

[cols="m,n,o", options="header"]
|============================================================================================================================
|Phase Name|Actions|Next Phase
|SCHEDULED|Initial phase after creation.|PREPARING
|PREPARING|Creation of task directories, obtaining and extracting application package and performing variable substitution.|STAGING
|STAGING|Provide the data of all input digital objects in the input directory of the task.|PROCESSING
|PROCESSING|Use the concrete handler implementation to execute/submit and monitor the application execution.|INGEST
|INGEST|Ingest the data located in the task output directory as a new digital object.|CLEANUP
|CLEANUP|Remove all task directories and their contents.|-
|============================================================================================================================

Each phase has a PHASE_SUCCESSFUL and a PHASE_FAILED state. If a phase has been completed successfully in during the last handler execution cycle it will enter the next phase in the next cycle. If the 
phase execution has failed, the task has to be reset to the last phase's SUCCESSFUL state manually in order to reattempt the phase execution. The actual execution of tasks is triggered either via a Cron job
executing the `DataWorkflowTrigger` script of by creating an appropriate job schedule using the <<ChapterJobScheduling,AdminUI>>. 

[NOTE]
For each handled workflow task only one (or no) state transition will occur during a single Cron/job schedule execution. Possible transitions are depicted in figure <<FigureTaskStatus,TaskStatus>>. In case of data transfer
tasks, e.g. Staging and Ingest, the phase might be entered multiple times as long as the data transfer operation has not finished, yet. All other phases are implemented in a synchronous way so the according Cron/job schedule
execution won't finish until the phase is either in its SUCCESSFUL or FAILED state.

[[FigureTaskStatus]]
.Task state transitions of data workflow tasks.
image::TaskStatus.png[TaskStatus]

Summarizing, the Data Workflow Service offer a great potential for processing digital objects in an automated way including the tracking of provenance information for better reproducability. For more details
on how to setup execution environments, workflow tasks and triggering them, please refer to the chapters <<ChapterInstallation,Installation>> of KIT Data Manager and <<ChapterSettings,Settings>> 
of the Administration UI.
